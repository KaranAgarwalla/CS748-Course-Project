{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS748_DQN_v5.ipynb","provenance":[{"file_id":"1oTWxjH764-ktun1qBMCZbqjTmOzPob-Q","timestamp":1609395338359}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HdQevZXkcJnl","executionInfo":{"status":"ok","timestamp":1613541577969,"user_tz":-330,"elapsed":1513,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}},"outputId":"987e2f5e-14d9-4a98-a544-84e785c948bc"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXBRYWJ8dyQ7","executionInfo":{"status":"ok","timestamp":1613541578363,"user_tz":-330,"elapsed":1896,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}}},"source":["#CONSTANTS\n","TRAIN           = None           # Boolean value indicating whether the model is to be trained or tested\n","GAME            = None           # Name of Game\n","ENV_NAME        = None           # Name of the environment in ALE: ENV_NAME = f'{args.game}Deterministic-v{args.version}'\n","ENV_FRAME_SHAPE = [210, 160, 3]  # Shape of frames in the environment\n","FRAME_SKIP      = None           # Count of frame-skip value; FRAME_SKIP = 1 means no frame skipping "],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#CONTROL PARAMETERS\n","MAX_EPISODE_LENGTH = 72000       # Equivalent of 20 minutes of gameplay at 60 frames per second\n","SAVE_FREQUENCY = 100000          # Model saved after every SAVE_FREQUENCY timesteps\n","EVAL_FREQUENCY = 800000          # Number of time_steps between evaluations\n","EVAL_STEPS = int(MAX_EPISODE_LENGTH/FRAME_SKIP)  # Number of time_steps for one evaluation\n","NETW_UPDATE_FREQ = min(10000*FRAME_SKIP, 160000) # Number of time_steps between updating the target network. \n","                                 # According to Mnih et al. 2015 this is measured in the number of \n","                                 # parameter updates (every four actions), however, in the \n","                                 # DeepMind code, it is clearly measured in the number\n","                                 # of actions the agent choses\n","                                 # set to min(10000*FRAME_SKIP, 160000)\n","DISCOUNT_FACTOR = 0.99           # gamma in the Bellman equation\n","REPLAY_MEMORY_START_SIZE = 200000# Number of completely random actions, \n","                                 # before the agent starts learning\n","MAX_STEPS = 50000000             # Total number of frames the agent sees \n","MEMORY_SIZE = 500000             # Number of transitions stored in the replay memory\n","NO_OP_STEPS = 10                 # Number of 'NOOP' or 'FIRE' actions at the beginning of an \n","                                 # evaluation episode\n","UPDATE_FREQ = max(FRAME_SKIP, 16)# Every four actions a gradient descend step is performed: set to max(FRAME_SKIP, 16)\n","HIDDEN = 1024                    # Number of filters in the final convolutional layer. The output \n","                                 # has the shape (1,1,1024) which is split into two streams. Both \n","                                 # the advantage stream and value stream have the shape \n","                                 # (1,1,512). This is slightly different from the original \n","                                 # implementation but tests I did with the environment Pong \n","                                 # have shown that this way the score increases more quickly\n","LEARNING_RATE = 0.00025          # Set to 0.00025 in Pong for quicker results. \n","                                 # Hessel et al. 2017 used 0.0000625\n","BS = 32                          # Batch size\n","AGENT_HISTORY_LENGTH = 4         # Number of frames stacked together to create a state\n","FRACTION_GPU = 0.4               # If running multiple instances on same GPU, reduce it to 0.4 else 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# OBJECT VARIABLES\n","MAIN_DQN        = None\n","TARGET_DQN      = None\n","init            = None\n","saver           = None\n","MAIN_DQN_VARS   = None\n","atari           = None\n","TARGET_DQN_VARS = None\n","\n","# PATH VARIABLES\n","PATH            = None\n","SUMMARIES       = None\n","RUNID           = None"]},{"cell_type":"code","metadata":{"id":"ZvgrVh9Wd9W8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613541580321,"user_tz":-330,"elapsed":3849,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}},"outputId":"5bc56baa-0cde-4777-80ac-4141aee2335d"},"source":["import os\n","import random\n","import gym\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import numpy as np\n","import imageio\n","from skimage.transform import resize\n","import time\n","import warnings"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qFNCZIoveOkX","executionInfo":{"status":"ok","timestamp":1613541580322,"user_tz":-330,"elapsed":3841,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}}},"source":["class FrameProcessor:\n","    \"\"\"Resizes and converts RGB Atari frames to grayscale\"\"\"\n","    def __init__(self, frame_height=84, frame_width=84):\n","        \"\"\"\n","        Args:\n","            frame_height: Integer, Height of a frame of an Atari game\n","            frame_width: Integer, Width of a frame of an Atari game\n","        \"\"\"\n","        self.frame_height = frame_height\n","        self.frame_width = frame_width\n","        self.frame = tf.placeholder(shape=ENV_FRAME_SHAPE, dtype=tf.uint8)\n","        self.processed = tf.image.rgb_to_grayscale(self.frame)\n","        self.processed = tf.image.crop_to_bounding_box(self.processed, 34, 0, 160, 160)\n","        self.processed = tf.image.resize_images(self.processed, [self.frame_height, self.frame_width], \n","                                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    def __call__(self, session, frame):\n","        \"\"\"\n","        Args:\n","            session: A Tensorflow session object\n","            frame: A ENV_FRAME_SHAPE frame of an Atari game in RGB\n","        Returns:\n","            A processed (frame_height, frame_width, 1) frame in grayscale\n","        \"\"\"\n","        return session.run(self.processed, feed_dict={self.frame:frame})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUStEcqj92Eq","executionInfo":{"status":"ok","timestamp":1613541580323,"user_tz":-330,"elapsed":3835,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}}},"source":["class DQN(object):\n","    \"\"\"Implements a Deep Q Network\"\"\"\n","    \n","    # pylint: disable=too-many-instance-attributes\n","    \n","    def __init__(self, n_actions, hidden=HIDDEN, learning_rate=LEARNING_RATE, \n","                 frame_height=84, frame_width=84, agent_history_length=AGENT_HISTORY_LENGTH):\n","        \"\"\"\n","        Args:\n","            n_actions: Integer, number of possible actions\n","            hidden: Integer, Number of filters in the final convolutional layer. \n","                    This is different from the DeepMind implementation\n","            learning_rate: Float, Learning rate for the Adam optimizer\n","            frame_height: Integer, Height of a frame of an Atari game\n","            frame_width: Integer, Width of a frame of an Atari game\n","            agent_history_length: Integer, Number of frames stacked together to create a state\n","        \"\"\"\n","        self.n_actions = n_actions\n","        self.hidden = hidden\n","        self.learning_rate = learning_rate\n","        self.frame_height = frame_height\n","        self.frame_width = frame_width\n","        self.agent_history_length = agent_history_length\n","        \n","        self.input = tf.placeholder(shape=[None, self.frame_height, \n","                                           self.frame_width, self.agent_history_length], \n","                                    dtype=tf.float32)\n","        # Normalizing the input\n","        self.inputscaled = self.input/255\n","        \n","        # Convolutional layers\n","        self.conv1 = tf.layers.conv2d(\n","            inputs=self.inputscaled, filters=32, kernel_size=[8, 8], strides=4,\n","            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n","            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv1')\n","        self.conv2 = tf.layers.conv2d(\n","            inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2, \n","            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n","            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv2')\n","        self.conv3 = tf.layers.conv2d(\n","            inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1, \n","            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n","            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv3')\n","        self.conv4 = tf.layers.conv2d(\n","            inputs=self.conv3, filters=hidden, kernel_size=[7, 7], strides=1, \n","            kernel_initializer=tf.variance_scaling_initializer(scale=2),\n","            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv4')\n","        \n","        # Splitting into value and advantage stream\n","        self.valuestream, self.advantagestream = tf.split(self.conv4, 2, 3)\n","        self.valuestream = tf.layers.flatten(self.valuestream)\n","        self.advantagestream = tf.layers.flatten(self.advantagestream)\n","        self.advantage = tf.layers.dense(\n","            inputs=self.advantagestream, units=self.n_actions,\n","            kernel_initializer=tf.variance_scaling_initializer(scale=2), name=\"advantage\")\n","        self.value = tf.layers.dense(\n","            inputs=self.valuestream, units=1, \n","            kernel_initializer=tf.variance_scaling_initializer(scale=2), name='value')\n","        \n","        # Combining value and advantage into Q-values as described above\n","        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n","        self.best_action = tf.argmax(self.q_values, 1)\n","        \n","        # The next lines perform the parameter update. This will be explained in detail later.\n","        \n","        # targetQ according to Bellman equation: \n","        # Q = r + gamma*max Q', calculated in the function learn()\n","        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n","        # Action that was performed\n","        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n","        # Q value of the action that was performed\n","        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.n_actions, dtype=tf.float32)), axis=1)\n","        \n","        # Parameter updates\n","        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions=self.Q))\n","        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n","        self.update = self.optimizer.minimize(self.loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldv4NasKVP76","executionInfo":{"status":"ok","timestamp":1613541580325,"user_tz":-330,"elapsed":3832,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}}},"source":["class ExplorationExploitationScheduler(object):\n","    \"\"\"Determines an action according to an epsilon greedy strategy with annealing epsilon\"\"\"\n","    def __init__(self, DQN, n_actions, eps_initial=1, eps_final=0.1, eps_final_step=0.01, \n","                 eps_evaluation=0.0, eps_annealing_steps=4000000, \n","                 replay_memory_start_size=REPLAY_MEMORY_START_SIZE, max_steps=MAX_STEPS):\n","        \"\"\"\n","        Args:\n","            DQN: A DQN object\n","            n_actions: Integer, number of possible actions\n","            eps_initial: Float, Exploration probability for the first \n","                replay_memory_start_size frames\n","            eps_final: Float, Exploration probability after \n","                replay_memory_start_size + eps_annealing_frames frames\n","            eps_final_frame: Float, Exploration probability after max_frames frames\n","            eps_evaluation: Float, Exploration probability during evaluation\n","            eps_annealing_frames: Int, Number of frames over which the \n","                exploration probabilty is annealed from eps_initial to eps_final\n","            replay_memory_start_size: Integer, Number of frames during \n","                which the agent only explores\n","            max_frames: Integer, Total number of frames shown to the agent\n","        \"\"\"\n","        self.n_actions = n_actions\n","        self.eps_initial = eps_initial\n","        self.eps_final = eps_final\n","        self.eps_final_step = eps_final_step\n","        self.eps_evaluation = eps_evaluation\n","        self.eps_annealing_steps = eps_annealing_steps\n","        self.replay_memory_start_size = replay_memory_start_size\n","        self.max_steps = max_steps\n","        \n","        # Slopes and intercepts for exploration decrease\n","        self.slope = -(self.eps_initial - self.eps_final)/self.eps_annealing_steps\n","        self.intercept = self.eps_initial - self.slope*self.replay_memory_start_size\n","        self.slope_2 = -(self.eps_final - self.eps_final_step)/(self.max_steps - self.eps_annealing_steps - self.replay_memory_start_size)\n","        self.intercept_2 = self.eps_final_step - self.slope_2*self.max_steps\n","        \n","        self.DQN = DQN\n","    \n","    def get_action(self, session, time_step, state, evaluation=False):\n","        \"\"\"\n","        Args:\n","            session: A tensorflow session object\n","            time_step: Integer, number of the current time_step\n","            state: A (84, 84, 4) sequence of frames of an Atari game in grayscale\n","            evaluation: A boolean saying whether the agent is being evaluated\n","        Returns:\n","            An integer between 0 and n_actions - 1 determining the action the agent perfoms next\n","        \"\"\"\n","        if evaluation:\n","            eps = self.eps_evaluation\n","        elif time_step < self.replay_memory_start_size:\n","            eps = self.eps_initial\n","        elif time_step >= self.replay_memory_start_size and time_step < self.replay_memory_start_size + self.eps_annealing_steps:\n","            eps = self.slope*time_step + self.intercept\n","        elif time_step >= self.replay_memory_start_size + self.eps_annealing_steps:\n","            eps = self.slope_2*time_step + self.intercept_2\n","        \n","        if np.random.rand(1) < eps:\n","            return np.random.randint(0, self.n_actions)\n","        return session.run(self.DQN.best_action, feed_dict={self.DQN.input:[state]})[0]  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ugL21cCxZGAj","executionInfo":{"status":"ok","timestamp":1613541580327,"user_tz":-330,"elapsed":3827,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}}},"source":["class ReplayMemory(object):\n","    \"\"\"Replay Memory that stores the last size=1,000,000 transitions\"\"\"\n","    def __init__(self, size=MEMORY_SIZE, frame_height=84, frame_width=84, \n","                 agent_history_length=AGENT_HISTORY_LENGTH, batch_size=BS):\n","        \"\"\"\n","        Args:\n","            size: Integer, Number of stored transitions\n","            frame_height: Integer, Height of a frame of an Atari game\n","            frame_width: Integer, Width of a frame of an Atari game\n","            agent_history_length: Integer, Number of frames stacked together to create a state\n","            batch_size: Integer, Number if transitions returned in a minibatch\n","        \"\"\"\n","        self.size = size\n","        self.frame_height = frame_height\n","        self.frame_width = frame_width\n","        self.agent_history_length = agent_history_length\n","        self.batch_size = batch_size\n","        self.count = 0\n","        self.current = 0\n","        \n","        # Pre-allocate memory\n","        self.actions = np.empty(self.size, dtype=np.int32)\n","        self.rewards = np.empty(self.size, dtype=np.float32)\n","        self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n","        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n","        \n","        # Pre-allocate memory for the states and new_states in a minibatch\n","        self.states = np.empty((self.batch_size, self.agent_history_length, \n","                                self.frame_height, self.frame_width), dtype=np.uint8)\n","        self.new_states = np.empty((self.batch_size, self.agent_history_length, \n","                                    self.frame_height, self.frame_width), dtype=np.uint8)\n","        self.indices = np.empty(self.batch_size, dtype=np.int32)\n","        \n","    def add_experience(self, action, frame, reward, terminal):\n","        \"\"\"\n","        Args:\n","            action: An integer between 0 and env.action_space.n - 1 \n","                determining the action the agent perfomed\n","            frame: A (84, 84, 1) frame of an Atari game in grayscale\n","            reward: A float determining the reward the agend received for performing an action\n","            terminal: A bool stating whether the episode terminated\n","        \"\"\"\n","        if frame.shape != (self.frame_height, self.frame_width):\n","            raise ValueError('Dimension of frame is wrong!')\n","        self.actions[self.current] = action\n","        self.frames[self.current, ...] = frame\n","        self.rewards[self.current] = reward\n","        self.terminal_flags[self.current] = terminal\n","        self.count = max(self.count, self.current+1)\n","        self.current = (self.current + 1) % self.size\n","             \n","    def _get_state(self, index):\n","        if self.count is 0:\n","            raise ValueError(\"The replay memory is empty!\")\n","        if index < self.agent_history_length - 1:\n","            raise ValueError(\"Index must be min 3\")\n","        return self.frames[index-self.agent_history_length+1:index+1, ...]\n","        \n","    def _get_valid_indices(self):\n","        for i in range(self.batch_size):\n","            while True:\n","                index = random.randint(self.agent_history_length, self.count - 1)\n","                if index < self.agent_history_length:\n","                    continue\n","                if index >= self.current and index - self.agent_history_length <= self.current:\n","                    continue\n","                if self.terminal_flags[index - self.agent_history_length:index].any():\n","                    continue\n","                break\n","            self.indices[i] = index\n","            \n","    def get_minibatch(self):\n","        \"\"\"\n","        Returns a minibatch of self.batch_size = 32 transitions\n","        \"\"\"\n","        if self.count < self.agent_history_length:\n","            raise ValueError('Not enough memories to get a minibatch')\n","        \n","        self._get_valid_indices()\n","            \n","        for i, idx in enumerate(self.indices):\n","            self.states[i] = self._get_state(idx - 1)\n","            self.new_states[i] = self._get_state(idx)\n","        \n","        return np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)),self.terminal_flags[self.indices]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SX4y6ZCle_kH","executionInfo":{"status":"ok","timestamp":1613541580328,"user_tz":-330,"elapsed":3822,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}}},"source":["def learn(session, replay_memory, main_dqn, target_dqn, batch_size, gamma):\n","    \"\"\"\n","    Args:\n","        session: A tensorflow sesson object\n","        replay_memory: A ReplayMemory object\n","        main_dqn: A DQN object\n","        target_dqn: A DQN object\n","        batch_size: Integer, Batch size\n","        gamma: Float, discount factor for the Bellman equation\n","    Returns:\n","        loss: The loss of the minibatch, for tensorboard\n","    Draws a minibatch from the replay memory, calculates the \n","    target Q-value that the prediction Q-value is regressed to. \n","    Then a parameter update is performed on the main DQN.\n","    \"\"\"\n","    # Draw a minibatch from the replay memory\n","    states, actions, rewards, new_states, terminal_flags = replay_memory.get_minibatch()    \n","    # The main network estimates which action is best (in the next \n","    # state s', new_states is passed!) \n","    # for every transition in the minibatch\n","    arg_q_max = session.run(main_dqn.best_action, feed_dict={main_dqn.input:new_states})\n","    # The target network estimates the Q-values (in the next state s', new_states is passed!) \n","    # for every transition in the minibatch\n","    q_vals = session.run(target_dqn.q_values, feed_dict={target_dqn.input:new_states})\n","    double_q = q_vals[range(batch_size), arg_q_max]\n","    # Bellman equation. Multiplication with (1-terminal_flags) makes sure that \n","    # if the game is over, targetQ=rewards\n","    target_q = rewards + (gamma*double_q * (1-terminal_flags))\n","    # Gradient descend step to update the parameters of the main network\n","    loss, _ = session.run([main_dqn.loss, main_dqn.update], \n","                          feed_dict={main_dqn.input:states, \n","                                     main_dqn.target_q:target_q, \n","                                     main_dqn.action:actions})\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HthyLliTg2I7","executionInfo":{"status":"ok","timestamp":1613541580328,"user_tz":-330,"elapsed":3814,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}}},"source":["class TargetNetworkUpdater(object):\n","    \"\"\"Copies the parameters of the main DQN to the target DQN\"\"\"\n","    def __init__(self, main_dqn_vars, target_dqn_vars):\n","        \"\"\"\n","        Args:\n","            main_dqn_vars: A list of tensorflow variables belonging to the main DQN network\n","            target_dqn_vars: A list of tensorflow variables belonging to the target DQN network\n","        \"\"\"\n","        self.main_dqn_vars = main_dqn_vars\n","        self.target_dqn_vars = target_dqn_vars\n","\n","    def _update_target_vars(self):\n","        update_ops = []\n","        for i, var in enumerate(self.main_dqn_vars):\n","            copy_op = self.target_dqn_vars[i].assign(var.value())\n","            update_ops.append(copy_op)\n","        return update_ops\n","            \n","    def __call__(self, sess):\n","        \"\"\"\n","        Args:\n","            sess: A Tensorflow session object\n","        Assigns the values of the parameters of the main network to the \n","        parameters of the target network\n","        \"\"\"\n","        update_ops = self._update_target_vars()\n","        for copy_op in update_ops:\n","            sess.run(copy_op)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0Py1Z8Sh4IG","executionInfo":{"status":"ok","timestamp":1613541580329,"user_tz":-330,"elapsed":3809,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}}},"source":["def generate_gif(frame_number, frames_for_gif, reward, path):\n","    \"\"\"\n","        Args:\n","            frame_number: Integer, determining the number of the current frame\n","            frames_for_gif: A sequence of (210, 160, 3) frames of an Atari game in RGB\n","            reward: Integer, Total reward of the episode that es ouputted as a gif\n","            path: String, path where gif is saved\n","    \"\"\"\n","    for idx, frame_idx in enumerate(frames_for_gif): \n","        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), \n","                                     preserve_range=True, order=0).astype(np.uint8)\n","        \n","    imageio.mimsave(f'{path}/ATARI_frame_{frame_number}_reward_{reward}.gif', \n","                    frames_for_gif, duration=1/30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GgVVoeoPiu1C","executionInfo":{"status":"ok","timestamp":1613541580330,"user_tz":-330,"elapsed":3805,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}}},"source":["class Atari(object):\n","    \"\"\"Wrapper for the environment provided by gym\"\"\"\n","    def __init__(self, envName, no_op_steps=NO_OP_STEPS, agent_history_length=AGENT_HISTORY_LENGTH, frameskip=FRAME_SKIP):\n","        self.env = gym.make(envName, frameskip=frameskip)\n","        self.process_frame = FrameProcessor()\n","        self.state = None\n","        self.last_lives = 0\n","        self.no_op_steps = no_op_steps\n","        self.agent_history_length = agent_history_length\n","        self.frameskip = frameskip\n","\n","    def reset(self, sess, evaluation=False):\n","        \"\"\"\n","        Args:\n","            sess: A Tensorflow session object\n","            evaluation: A boolean saying whether the agent is evaluating or training\n","        Resets the environment and stacks four frames ontop of each other to \n","        create the first state\n","        \"\"\"\n","        frame = self.env.reset()\n","        self.last_lives = 0\n","        terminal_life_lost = True # Set to true so that the agent starts \n","                                  # with a 'FIRE' action when evaluating\n","        if evaluation:\n","            for _ in range(random.randint(1, self.no_op_steps)):\n","                frame, _, _, _ = self.env.step(1) # Action 'Fire'\n","        processed_frame = self.process_frame(sess, frame)   # (★★★)\n","        self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n","        \n","        return terminal_life_lost\n","\n","    def step(self, sess, action):\n","        \"\"\"\n","        Args:\n","            sess: A Tensorflow session object\n","            action: Integer, action the agent performs\n","        Performs an action and observes the reward and terminal state from the environment\n","        \"\"\"\n","        new_frame, reward, terminal, info = self.env.step(action)  # (5★)\n","            \n","        if info['ale.lives'] < self.last_lives:\n","            terminal_life_lost = True\n","        else:\n","            terminal_life_lost = terminal\n","        self.last_lives = info['ale.lives']\n","        \n","        processed_new_frame = self.process_frame(sess, new_frame)   # (6★)\n","        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2) # (6★)   \n","        self.state = new_state\n","        \n","        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hsanvL6wk2PQ","executionInfo":{"status":"ok","timestamp":1613541580331,"user_tz":-330,"elapsed":3799,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}}},"source":["def clip_reward(reward):\n","    return np.sign(reward)"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train():\n","    \"\"\"Contains the training and evaluation loops\"\"\"\n","    my_replay_memory = ReplayMemory(size=MEMORY_SIZE, batch_size=BS)   # (★)\n","    update_networks = TargetNetworkUpdater(MAIN_DQN_VARS, TARGET_DQN_VARS)\n","    \n","    explore_exploit_sched = ExplorationExploitationScheduler(\n","        MAIN_DQN, atari.env.action_space.n, \n","        replay_memory_start_size=REPLAY_MEMORY_START_SIZE, \n","        max_steps=MAX_STEPS)\n","\n","    reward_per_01 = open(PATH + '/rewards_every_episode.dat', 'a')\n","    reward_per_10 = open(PATH + '/rewards_every_10_episodes.dat', 'a')\n","    reward_eval_01= open(PATH + '/rewards_eval_every_episodes.dat', 'a')\n","    reward_eval   = open(PATH + '/rewards_eval.dat', 'a')\n","    with tf.Session(config=config) as sess:\n","        sess.run(init)\n","        \n","        time_step = 0\n","        episode_number = 0\n","        frame_number = 0\n","        rewards = []\n","        loss_list = []\n","        \n","        while time_step < MAX_STEPS:\n","            \n","            ########################\n","            ####### Training #######\n","            ########################\n","\n","            epoch_steps = 0\n","            while epoch_steps < EVAL_FREQUENCY:\n","                terminal_life_lost = atari.reset(sess)\n","                episode_reward_sum = 0\n","                episode_iter = 0\n","                while episode_iter < MAX_EPISODE_LENGTH:\n","                    episode_iter += FRAME_SKIP # (4★)\n","                    action = explore_exploit_sched.get_action(sess, frame_number, atari.state) # (5★)\n","                    processed_new_frame, reward, terminal, terminal_life_lost, _ = atari.step(sess, action)  \n","                    time_step += FRAME_SKIP\n","                    frame_number += 1\n","                    epoch_steps += FRAME_SKIP\n","                    episode_reward_sum += reward\n","                    \n","                    # Clip the reward\n","                    clipped_reward = clip_reward(reward)\n","                    \n","                    # (7★) Store transition in the replay memory\n","                    my_replay_memory.add_experience(action=action, \n","                                                    frame=processed_new_frame[:, :, 0],\n","                                                    reward=clipped_reward, \n","                                                    terminal=terminal_life_lost)   \n","                    \n","                    ## Perform Gradient Descent\n","                    if time_step % UPDATE_FREQ == 0 and time_step > REPLAY_MEMORY_START_SIZE:\n","                        loss = learn(sess, my_replay_memory, MAIN_DQN, TARGET_DQN,\n","                                     BS, gamma = DISCOUNT_FACTOR) # (8★)\n","                        loss_list.append(loss)\n","\n","                    ## Update the Target Network\n","                    if time_step % NETW_UPDATE_FREQ == 0 and time_step > REPLAY_MEMORY_START_SIZE:\n","                        update_networks(sess) # (9★)\n","                    \n","                    ## Save the network parameters\n","                    if time_step % SAVE_FREQUENCY == 0:\n","                        saver.save(sess, PATH+'/my_model', global_step=time_step)\n","        \n","                    if terminal:\n","                        terminal = False\n","                        break\n","\n","                episode_number += 1\n","                rewards.append(episode_reward_sum)\n","                \n","                print(len(rewards), time_step, frame_number, episode_number, episode_reward_sum, file = reward_per_01)\n","                # Output the progress:\n","                if len(rewards) % 10 == 0:\n","                    print(len(rewards), frame_number, np.mean(rewards[-100:]))\n","                    print(len(rewards), time_step, frame_number, episode_number,\n","                            np.mean(rewards[-10:]), file=reward_per_10)\n","            \n","            ########################\n","            ###### Evaluation ######\n","            ########################\n","            terminal = True\n","            gif = True\n","            frames_for_gif = []\n","            eval_rewards = []\n","            evaluate_frame_number = 0\n","            \n","            for _ in range(EVAL_STEPS):\n","                if terminal:\n","                    terminal_life_lost = atari.reset(sess, evaluation=True)\n","                    episode_reward_sum = 0\n","                    terminal = False\n","               \n","                # Fire (action 1), when a life was lost or the game just started, \n","                # so that the agent does not stand around doing nothing. When playing \n","                # with other environments, you might want to change this...\n","                action = 1 if terminal_life_lost else explore_exploit_sched.get_action(sess, time_step,\n","                                                                                       atari.state, \n","                                                                                       evaluation=True)\n","                \n","                processed_new_frame, reward, terminal, terminal_life_lost, new_frame = atari.step(sess, action) ### A seperate Atari\n","                evaluate_frame_number += 1\n","                episode_reward_sum += reward\n","\n","                if gif: \n","                    frames_for_gif.append(new_frame)\n","                if terminal:\n","                    print(time_step, frame_number, episode_number, episode_reward_sum, file = reward_eval_01)\n","                    gif = False # Save only the first game of the evaluation as a gif\n","                    break\n","            \n","            ## Append the rewards\n","            eval_rewards.append(episode_reward_sum)\n","            print(\"Evaluation score:\\n\", np.mean(eval_rewards))       \n","            try:\n","                generate_gif(frame_number, frames_for_gif, eval_rewards[0], PATH)\n","            except IndexError:\n","                print(\"No evaluation game finished\")\n","            \n","            frames_for_gif = []\n","            \n","            print(time_step, frame_number, episode_number, np.mean(eval_rewards), file=reward_eval)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJKgyAock7TM","executionInfo":{"status":"ok","timestamp":1613541580855,"user_tz":-330,"elapsed":4317,"user":{"displayName":"Rishi Agarwal","photoUrl":"","userId":"13706671974881542882"}},"outputId":"8dfbf4c6-c3f5-400e-add2-038c4ac0fc6f"},"source":["tf.reset_default_graph()\n","atari       = Atari(ENV_NAME, no_op_steps=NO_OP_STEPS, frameskip=FRAME_SKIP)\n","# main DQN and target DQN networks:\n","with tf.variable_scope('mainDQN'):\n","    MAIN_DQN = DQN(atari.env.action_space.n, HIDDEN, LEARNING_RATE)   # (★★)\n","with tf.variable_scope('targetDQN'):\n","    TARGET_DQN = DQN(atari.env.action_space.n, HIDDEN)                # (★★)\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver(max_to_keep=100000)    \n","\n","MAIN_DQN_VARS = tf.trainable_variables(scope='mainDQN')\n","TARGET_DQN_VARS = tf.trainable_variables(scope='targetDQN')\n","\n","# Google Drive Path\n","PATH      = f'/content/drive/MyDrive/DQN-Train/{GAME}-{FRAME_SKIP}'\n","# Google Cloud Path\n","# PATH      = f'DQN/{GAME}-{FRAME_SKIP}' # Gifs and checkpoints will be saved here\n","SUMMARIES = 'summaries'                # logdir for tensorboard\n","RUNID     = 1\n","while os.path.exists(PATH + '/run_' + str(RUNID)):\n","    RUNID += 1\n","RUNID     = '/run_' + str(RUNID)\n","PATH      = PATH + RUNID\n","SUMMARIES = PATH + '/' + SUMMARIES\n","os.makedirs(SUMMARIES, exist_ok=True)\n","print(f'The env {ENV_NAME} has the following {atari.env.action_space.n} \\\n","    actions: {atari.env.unwrapped.get_action_meanings()}')\n","if TRAIN:\n","    train()"],"execution_count":null,"outputs":[]}]}